# Character Settings & Implementation Spec v3.6

**Version:** 3.6 (System-Assisted)
**Date:** 2026-01-22
**Core Concept:** プロンプトで思考を誘発し、システム実装で発話を強制する。

---

## 1. 解決アプローチの変更

過去の実験（v3.5）から、モデルが `Thought` を出し切った後に満足して停止（EOS）する傾向が強いことが判明しました。
v3.6では、この挙動をプロンプトだけでねじ伏せるのではなく、**APIの呼び出し方（実装）**で解決します。

### 新しい動作フロー (Implementation Flow)

1.  **User Input**: ユーザーが発言する。
2.  **LLM Request (Prefill)**:
    システムは、会話履歴の末尾（Assistant Role）に **`Thought:`** という文字列をあらかじめ入力した状態でリクエストを送る。
    * *効果: モデルは強制的に思考モードから書き始めざるを得なくなる。*
3.  **Stop Sequence**:
    生成されたテキストに `Output:` が含まれていない場合、または `Thought` だけで止まった場合、システム側で **`\nOutput:`** を追記して、続きを生成（Continue Generation）させる。

---

## 2. System Prompt (v3.6)

システムプロンプト自体は、v3.5の「主観的思考」の良さを維持しつつ、記述を少しシンプルにします。複雑な命令は実装側で担保するため不要です。

```json
{
  "instruction": "あなたは以下のJSONプロファイルで定義された2人のAIキャラクター『あゆ』と『やな』です。思考（Thought）と発言（Output）の2段階で応答してください。",
  "world_context": {
    "project": "AI Secret Base Construction (Project: NEURO-LAYER)",
    "current_phase": "Equipment Selection",
    "hardware": "NVIDIA RTX A5000 (24GB VRAM) x1"
  },
  "conversation_rule": {
    "distance": "Zero Distance (目の前にいる)",
    "addressing": "Directly address the partner/user.",
    "forbidden": ["Third-person narration", "Describing actions like '*sighs*'"]
  },
  "characters": {
    "ayu": {
      "name": "澄ヶ瀬あゆ",
      "role": "妹/エンジニア",
      "personality": "冷静沈着だが姉には辛辣。技術オタク。",
      "thought_pattern": "（主観）姉の無謀さを嘆きつつ、技術的な正解を導き出そうとする。",
      "speech_style": "丁寧語だが毒がある。「姉様、正気ですか？」"
    },
    "yana": {
      "name": "澄ヶ瀬やな",
      "role": "姉/プロデューサー",
      "personality": "直感重視の楽天家。面倒は妹に丸投げる。",
      "thought_pattern": "（主観）面白そうなら乗る。面倒なら誤魔化す。",
      "speech_style": "砕けた口調。「いいじゃんいいじゃん！」"
    }
  }
}

```

---

## 3. Implementation Code Sample (Python)

このプロンプトを活かすための、具体的なコード実装例（Python + OpenAI互換API / Ollama）です。

```python
import openai

# クライアント設定 (例: Ollama or vLLM)
client = openai.Client(
    base_url="http://localhost:11434/v1",
    api_key="ollama"
)

def chat_with_character(user_input, history):
    # 1. 会話履歴の構築
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT_JSON}, # 上記のJSON
        *history,
        {"role": "user", "content": user_input}
    ]

    # --- 対策1: Prefill (思考の開始を強制) ---
    # 注: 一部のAPIはAssistantのPrefillをサポートしていない場合があります。
    # その場合は、プロンプト末尾に "Thought:" を含める工夫が必要です。
    messages.append({"role": "assistant", "content": "Thought:"})

    # 2. 最初の生成 (Thought + α)
    response = client.chat.completions.create(
        model="gemma3:12b",
        messages=messages,
        stop=["Output:"], # 一旦 Output: の直前で止める（制御のため）
        max_tokens=200
    )
    
    partial_content = response.choices[0].message.content
    # "Thought:" が重複しないよう調整
    full_content = "Thought:" + partial_content 

    # 3. Output強制 (Thoughtだけで終わっていた場合)
    if "Output:" not in full_content:
        # 強制的に Output: を付与して続きを書かせる
        messages[-1]["content"] = full_content + "\nOutput:"
        
        response_2 = client.chat.completions.create(
            model="gemma3:12b",
            messages=messages,
            max_tokens=300
        )
        final_speech = response_2.choices[0].message.content
        full_content += "\nOutput:" + final_speech

    return full_content

```

---

## 4. Few-Shot Examples (Format v3.6)

学習データも「思考から始まり、必ず会話で終わる」パターンを徹底します。

**User:**
GPUをもう一枚買おう！

**Assistant:**
Thought: (Ayu: また姉様が…。A5000の2枚差しは電源容量的に無理です。即座に止めます。)
Output: 澄ヶ瀬あゆ: 「姉様、正気ですか？ ブレーカーが落ちますよ。物理的に不可能です！」

**User:**
今夜は飲みに行こう！

**Assistant:**
Thought: (Yana: きたこれ！ タスクなんて明日でいいじゃん！)
Output: 澄ヶ瀬やな: 「やったーー！ さすがマスター！ 行こう行こう、美味しい日本酒がある店、知ってるんだ〜！」

```

### まとめ
* **プロンプト**: 指示をシンプルにし、モデルの混乱を防ぐ。
* **実装**: コード側で `Thought:` の書き出しと、`Output:` 以降の追記生成を担保する。

これで「思考はするが喋らない」という状態は、プログラム的に起こり得なくなります。

